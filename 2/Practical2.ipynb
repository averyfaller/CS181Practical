{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "TRAIN_DIR = \"train\"\n",
    "TEST_DIR = \"test\"\n",
    "\n",
    "call_set = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UTILITIES\n",
    "# these are the fifteen malware classes we're looking for\n",
    "malware_classes = [\"Agent\", \"AutoRun\", \"FraudLoad\", \"FraudPack\", \"Hupigon\", \"Krap\",\n",
    "           \"Lipler\", \"Magania\", \"None\", \"Poison\", \"Swizzor\", \"Tdss\",\n",
    "           \"VB\", \"Virut\", \"Zbot\"]\n",
    "\n",
    "# a function for writing predictions in the required format\n",
    "def write_predictions(predictions, ids, outfile):\n",
    "    \"\"\"\n",
    "    assumes len(predictions) == len(ids), and that predictions[i] is the\n",
    "    index of the predicted class with the malware_classes list above for \n",
    "    the executable corresponding to ids[i].\n",
    "    outfile will be overwritten\n",
    "    \"\"\"\n",
    "    with open(outfile,\"w+\") as f:\n",
    "        # write header\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i, history_id in enumerate(ids):\n",
    "            f.write(\"%s,%d\\n\" % (history_id, predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_set(tree):\n",
    "    for el in tree.iter():\n",
    "        call = el.tag\n",
    "        call_set.add(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data_matrix(start_index, end_index, direc=\"train\"):\n",
    "    X = None\n",
    "    classes = []\n",
    "    ids = [] \n",
    "    i = -1\n",
    "    for datafile in os.listdir(direc):\n",
    "        if datafile == '.DS_Store':\n",
    "            continue\n",
    "\n",
    "        i += 1\n",
    "        if i < start_index:\n",
    "            continue \n",
    "        if i >= end_index:\n",
    "            break\n",
    "\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str, clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(malware_classes.index(clazz))\n",
    "\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        add_to_set(tree)\n",
    "        this_row = call_feats(tree)\n",
    "        if X is None:\n",
    "            X = this_row \n",
    "        else:\n",
    "            X = np.vstack((X, this_row))\n",
    "\n",
    "    return X, np.array(classes), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_calls = [\n",
    "    'processes', 'query_value', 'read_section', 'read_section_names', 'read_value', \n",
    "        'recv_socket', 'remove_directory', 'revert_to_self', 'send_socket', \n",
    "        'set_file_attributes', 'set_file_time', 'set_system_time', 'set_thread_context', \n",
    "        'set_value', 'set_windows_hook', 'show_window', 'sleep', 'start_service', 'thread',\n",
    "        'trimmed_bytes', 'unload_driver', 'vm_allocate', 'vm_mapviewofsection', 'vm_protect',\n",
    "        'vm_read', 'vm_write', 'write_value', 'open_key', 'create_open_file', 'get_host_by_name', \n",
    "        'load_dll', 'create_mutex', 'load_image', 'create_file', 'get_file_attributes', 'open_process',\n",
    "        'find_window', 'com_create_instance', 'destroy_window', 'impersonate_user', 'dump_line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def call_feats(tree):\n",
    "    \n",
    "#     bigram_calls = []\n",
    "#     for a_call in good_calls:\n",
    "#         for b_call in good_calls:\n",
    "#             bigram_calls.append(a_call + '_' + b_call)\n",
    "            \n",
    "#     combined_calls = good_calls + bigram_calls\n",
    "\n",
    "    call_counter = {}\n",
    "    last_call='start'\n",
    "    for el in tree.iter():\n",
    "        call = el.tag\n",
    "        # Add specific call counts\n",
    "        if call not in call_counter:\n",
    "            call_counter[call] = 1\n",
    "        else:\n",
    "            call_counter[call] += 1\n",
    "            \n",
    "#         # Bigram Calls\n",
    "#         combo = last_call + '_' + call\n",
    "#         if combo not in call_counter:\n",
    "#             call_counter[combo] = 1\n",
    "#         #else:\n",
    "#         #    call_counter[combo] += 1\n",
    "#         last_call = call\n",
    "            \n",
    "    call_feat_array = np.zeros(len(good_calls))\n",
    "    for i in range(len(good_calls)):\n",
    "        call = good_calls[i]\n",
    "        call_feat_array[i] = 0\n",
    "        if call in call_counter:\n",
    "            call_feat_array[i] = call_counter[call]\n",
    "\n",
    "    return call_feat_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix (training set):\n",
      "[[   1.    9.    0. ...,    1.    0.    0.]\n",
      " [   1.   14.    0. ...,    1.    0.    0.]\n",
      " [   1.  242.    0. ...,   42.    0.    0.]\n",
      " ..., \n",
      " [   1.  242.    0. ...,   42.    0.    0.]\n",
      " [   1.    0.    0. ...,    0.    0.    0.]\n",
      " [   1.  534.    0. ...,    0.    3.   26.]]\n",
      "Classes (training set):\n",
      "[ 8  8  8 ...,  8  2 10]\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA\n",
    "X_train_all, t_train_all, train_all_ids = create_data_matrix(0, 3086, TRAIN_DIR)\n",
    "X_train, X_valid, t_train, t_valid = train_test_split(X_train_all, t_train_all, test_size=0.20, random_state=37)\n",
    "\n",
    "print 'Data matrix (training set):'\n",
    "print X_train\n",
    "print 'Classes (training set):'\n",
    "print t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING DATA\n",
    "X_test_all, t_test_all, test_all_ids = create_data_matrix(0, 3724, TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Score of validation set: 0.760518\n"
     ]
    }
   ],
   "source": [
    "# Try SVM\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, t_train)\n",
    "print \"SVM\"\n",
    "print \"Score of validation set: %f\" % clf.score(X_valid, t_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests\n",
      "For n = 1, score was: 0.830097\n",
      "For n = 5, score was: 0.865696\n",
      "For n = 7, score was: 0.864078\n",
      "For n = 8, score was: 0.875405\n",
      "For n = 9, score was: 0.875405\n",
      "For n = 10, score was: 0.870550\n",
      "For n = 11, score was: 0.877023\n",
      "For n = 12, score was: 0.888350\n",
      "For n = 13, score was: 0.877023\n",
      "For n = 14, score was: 0.877023\n",
      "For n = 15, score was: 0.878641\n",
      "For n = 20, score was: 0.883495\n",
      "For n = 25, score was: 0.885113\n",
      "For n = 30, score was: 0.883495\n",
      "For n = 35, score was: 0.868932\n",
      "For n = 40, score was: 0.877023\n",
      "For n = 50, score was: 0.873786\n",
      "For n = 100, score was: 0.875405\n",
      "For n = 200, score was: 0.877023\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "print \"Random Forests\"\n",
    "ns = [1,5,7,8,9,10,11,12,13,14,15,20,25,30,35,40,50,100,200]\n",
    "for n in ns:\n",
    "    rf = RandomForestClassifier(n_estimators=n)\n",
    "    rf.fit(X_train, t_train)\n",
    "    print \"For n = %d, score was: %f\" % (n, rf.score(X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was: 0.870550\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=12, min_samples_split=3)\n",
    "rf.fit(X_train, t_train)\n",
    "print \"Score was: %f\" % (rf.score(X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was: 0.822006\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2',solver='newton-cg',max_iter=500)\n",
    "lr.fit(X_train, t_train)\n",
    "print \"Score was: %f\" % (lr.score(X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was: 0.818770\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1')\n",
    "lr.fit(X_train, t_train)\n",
    "print \"Score was: %f\" % (lr.score(X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Multiple input features cannot have the same target value.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-2735c4860ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#theta0=0.1, thetaL=.001, thetaU=1.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Score was: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Avery/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/gaussian_process.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    300\u001b[0m         if (np.min(np.sum(D, axis=1)) == 0.\n\u001b[1;32m    301\u001b[0m                 and self.corr != correlation.pure_nugget):\n\u001b[0;32m--> 302\u001b[0;31m             raise Exception(\"Multiple input features cannot have the same\"\n\u001b[0m\u001b[1;32m    303\u001b[0m                             \" target value.\")\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Multiple input features cannot have the same target value."
     ]
    }
   ],
   "source": [
    "gp = GaussianProcess()#theta0=0.1, thetaL=.001, thetaU=1.)\n",
    "gp.fit(X_train, t_train)\n",
    "print \"Score was: %f\" % (gp.score(X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was: 0.283172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, t_train)\n",
    "print \"Score was: %f\" % (clf.score(X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "For n = 1, score was: 0.836570\n",
      "For n = 2, score was: 0.838188\n",
      "For n = 3, score was: 0.839806\n",
      "For n = 4, score was: 0.844660\n",
      "For n = 5, score was: 0.839806\n",
      "For n = 6, score was: 0.834951\n",
      "For n = 7, score was: 0.834951\n",
      "For n = 8, score was: 0.834951\n",
      "For n = 10, score was: 0.831715\n",
      "For n = 20, score was: 0.813916\n",
      "For n = 25, score was: 0.802589\n",
      "For n = 30, score was: 0.789644\n",
      "For n = 50, score was: 0.775081\n",
      "For n = 100, score was: 0.745955\n",
      "For n = 200, score was: 0.674757\n"
     ]
    }
   ],
   "source": [
    "# Try KNeighborsClassifier\n",
    "ns = [1,2,3,4,5,6,7,8,10,20,25,30,50,100,200]\n",
    "kneighborsscores = []\n",
    "print \"KNeighborsClassifier\"\n",
    "for n in ns:\n",
    "    nn = KNeighborsClassifier(n_neighbors=n)\n",
    "    nn.fit(X_train, t_train)\n",
    "    score = nn.score(X_valid, t_valid)\n",
    "    kneighborsscores.append(score)\n",
    "    print \"For n = %d, score was: %f\" % (n, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was: 0.864078\n"
     ]
    }
   ],
   "source": [
    "kneighborsscores2 = []\n",
    "nn = KNeighborsClassifier(n_neighbors=6, weights='distance')\n",
    "nn.fit(X_train, t_train)\n",
    "score = nn.score(X_valid, t_valid)\n",
    "kneighborsscores2.append(score)\n",
    "print \"Score was: %f\" % (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was: 0.864078\n"
     ]
    }
   ],
   "source": [
    "kneighborsscores2 = []\n",
    "nn = KNeighborsClassifier(n_neighbors=6, weights='distance')\n",
    "nn.fit(X_train, t_train)\n",
    "score = nn.score(X_valid, t_valid)\n",
    "kneighborsscores2.append(score)\n",
    "print \"Score was: %f\" % (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score was: 0.745955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "rnc = RadiusNeighborsClassifier(radius=6,outlier_label=8, p=2)\n",
    "rnc.fit(X_train, t_train)\n",
    "print \"Score was: %f\" % (rnc.score(X_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions on test set for the best model\n",
    "\n",
    "nn = KNeighborsClassifier(n_neighbors=6, weights='distance', p=1)\n",
    "nn.fit(X_train_all, t_train_all)\n",
    "test_predictions = nn.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=12, min_samples_split=3)\n",
    "rf.fit(X_train_all, t_train_all)\n",
    "test_predictions = rf.predict(X_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_file(filename, ids, predictions):\n",
    "    zips = zip(ids, predictions)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i,p in enumerate(zips):\n",
    "            f.write(str(p[0]) + \",\" + str(p[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(\"prediction.csv\", test_all_ids, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00278ec420236020d6121dffe0cc20034422e7228.Lipler.xml\n",
      "[  1.00000000e+00   2.00400000e+03   1.00000000e+00   0.00000000e+00\n",
      "   5.10000000e+01   2.11000000e+02   1.00000000e+00   0.00000000e+00\n",
      "   1.70000000e+01   2.00000000e+01   1.80000000e+01   0.00000000e+00\n",
      "   0.00000000e+00   7.00000000e+01   2.80000000e+01   4.00000000e+00\n",
      "   2.54000000e+02   0.00000000e+00   4.10000000e+01   2.05000000e+02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   3.65000000e+02\n",
      "   0.00000000e+00   0.00000000e+00   3.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Run this to see what call_feats returns\n",
    "direc = \"train\"\n",
    "for idx, datafile in enumerate(os.listdir(direc)):\n",
    "    if idx == 1:\n",
    "        print datafile\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        feats = call_feats(tree)\n",
    "        \n",
    "#         for el in tree.iter():\n",
    "#             print el.tag\n",
    "        print feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
